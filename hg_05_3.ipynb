{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOC1+DOeayrXeq08u6YX9Wd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lonen8188/PythonAI/blob/5-3.Ensemble/hg_05_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 앙상블 : 단어 그대로 여러 단순한 모델을 결합하여 정확한 모델을 만드는 방법이다.\n",
        "# 정형데이터 : 지금까지 학습한 수치자료가 있는 값\n",
        "# 비정형데이터 : 데이터베이스나 엑셀로 표현하기 어려운 데이터(텍스트데이터, 디카사진, mp3 등.) -> 신경망 알고리즘\n",
        "# 랜덤 포레스트 : 결정 트리를 랜덤하게 만들어 결정트리(나무)숲 을 만듬 -> 최종 예측\n",
        "# 1000개의 샘플이 들어 있는 가방에서 100개을 샘플을 뽑을때 1개를 뽑고, 뽑앗던 1개를 다시 가방에 넣음\n",
        "# 중복된 샘플을 뽑을 수 있음 -> 부트스트랩 샘플이라고 함\n",
        "# 부트스트랩 : 데이터 세트에서 중복을 허용하여 데이터를 샘플링\n",
        "\n",
        "# 분류 모델인 : RandomForestClassifier는 기본적으로 전체 특성 개수의 제곱근만큼 특성을 선택\n",
        "# 즉 4개의 특성이 있다면 노드마드 2개를 랜덤하게 선택하여 사용\n",
        "# 다만 회귀 모델인 RandomForestRegressor는 전체 특성을 사용\n",
        "\n",
        "# 사이킷 런의 랜덤 포레스트는 기본적으로 100개의 결정 트리를 이런 방식으로 훈련한다.\n",
        "# 그 다음 분류일 때는 각 트리의 클래스별 확률을 평균하여 가장 높은 확율을 가진 클래스를 예측으로 삼는다.\n",
        "# 회귀일 때는 단순히 각 트리의 예측을 평균함.\n",
        "# 분류 : 샘플을 몇개의 클래스 중 하나로 분류하는 문제\n",
        "# 회귀 : 임의의 어떤 숫자를 예측하는 문제\n",
        "\n",
        "# 와인찾기 데이터\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "wine = pd.read_csv('https://bit.ly/wine_csv_data')\n",
        "\n",
        "data = wine[['alcohol', 'sugar', 'pH']].to_numpy()\n",
        "target = wine['class'].to_numpy()\n",
        "\n",
        "# 훈련세트와 테스트세트로 나눔\n",
        "train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "sdK6XAMwJ028"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_validate # 크로스 검증용\n",
        "from sklearn.ensemble import RandomForestClassifier # 100개의 결정 트리 사용\n",
        "\n",
        "rf = RandomForestClassifier(n_jobs=-1, random_state=42) # n_jobs=-1 모든 cpu 사용\n",
        "scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "# return_train_score=True 검증 점수와 훈련 세트에 대한 점수도 리턴\n",
        "\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "# train_score 과대 적합 0.9973541965122431"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sgdodl1XOHgT",
        "outputId": "83903995-ea92-49e5-dade-0e961a9d3557"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9973541965122431 0.8905151032797809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://gsbang.tistory.com/entry/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5-%EC%95%99%EC%83%81%EB%B8%94-%ED%95%99%EC%8A%B5-%EB%9E%9C%EB%8D%A4%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8Random-Forest#google_vignette\n",
        "\n",
        "# n_estimators: 랜덤 포레스트에서 사용할 의사결정나무의 개수입니다. 이 값이 크면 클수록 모델의 성능은 좋아질 수 있지만, 계산 시간이 늘어납니다. (기본값: 100)\n",
        "# criterion: 분할 품질을 측정하는 기능입니다. “gini”는 지니계수를 사용하고, “entropy”는 엔트로피를 사용합니다. (기본값: “gini”)\n",
        "# max_depth: 나무의 최대 깊이를 제어합니다. 이 값이 너무 크면 과적합(overfitting)의 위험이 있고, 너무 작으면 성능이 떨어질 수 있습니다. None으로 설정하면, 모든 리프 노드가 순수해질 때까지 노드가 확장됩니다. (기본값: None)\n",
        "# min_samples_split: 노드를 분할하는데 필요한 최소 샘플 수입니다. 이 값이 크면 나무의 깊이가 줄어들어 과적합을 방지할 수 있습니다. (기본값: 2)\n",
        "# min_samples_leaf: 리프 노드에 있어야 하는 최소 샘플 수입니다. 이 값도 과적합을 제어하는데 사용됩니다. (기본값: 1)\n",
        "# max_features: 각 노드에서 분할에 사용할 특성의 최대 수입니다. 이 값을 적절히 설정하여, 나무들 간의 상관관계를 줄일 수 있습니다. (기본값: \"auto\", 즉 sqrt(n_features))\n",
        "# bootstrap: 부트스트랩 샘플링을 사용할지 여부를 결정합니다. True로 설정하면, 부트스트랩 샘플링을 사용하며, False로 설정하면 전체 데이터셋을 사용합니다. (기본값: True)\n",
        "# oob_score: out-of-bag 샘플을 사용하여 일반화 정확도를 추정할지 여부를 결정합니다. (기본값: False)\n",
        "# (out-of-bag : 학습에 사용되지 않은 데이터)\n",
        "# n_jobs: 적합성과 예측을 위해 동시에 실행할 작업 수입니다. -1로 설정하면 모든 프로세서를 사용합니다. (기본값: None)\n",
        "# random_state: 랜덤성을 제어하는 데 사용되는 시드 값입니다. 이 값을 고정하여 실험의 재현성을 보장할 수 있습니다. (기본값: None)\n",
        "# verbose: 학습 과정에서 출력되는 텍스트 메시지의 상세 수준을 조절합니다. (기본값: 0)\n",
        "# class_weight: 클래스 가중치를 설정할 수 있습니다. 불균형한 데이터셋을 처리할 때 유용합니다. (기본값: None)\n",
        "\n",
        "rf.fit(train_input, train_target) # 훈련 후 특성 중요도 출력\n",
        "print(rf.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGGpr2WSOyD0",
        "outputId": "e27300a7-d27a-46cb-c5bd-bbbcf172430f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.23167441 0.50039841 0.26792718]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [0.12345626 0.86862934 0.0079144 ] LogisticRegression 이전 결과\n",
        "# 알콜도수   , 당도      , pH 비교 해보자.\n",
        "# 당도의 중요도가 감소하고, 알콜 도수와 pH 중요도가 상승함\n",
        "\n",
        "# 특성 일부를 랜덤하게 선택하여 결정 트리를 훈련 하기 때문임\n",
        "# RandomForestClassifier기능중에 자체적으로 모델을 평가하는 점수를 얻을 수 있다.\n",
        "# OOB(Out Of Bag)부트스트랩에 포함되지 않고 남은 샘플 -> 결정 트리 평가용(검증 세트로 활용)\n",
        "# oob_score=True\n",
        "rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)\n",
        "\n",
        "rf.fit(train_input, train_target)\n",
        "print(rf.oob_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZ5T1C71PzL1",
        "outputId": "12927eaf-9ca8-477b-8d62-75cfe03bcfcc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8934000384837406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 엑스트라 트리 : 100개의 결정 트리 훈련 -> 부트스트랩 샘플을 사용하지 않음(전체 훈련세트 사용)\n",
        "# 대신 노드를 분할할 때 가장 좋은 분할을 찾는 것이 아니라 무작위로 분할함!\n",
        "# 엑스트라 트리의 결정 트리 splitter='random'\n",
        "\n",
        "from sklearn.ensemble import ExtraTreesClassifier # ExtraTreesClassifier 엑스트라 트리\n",
        "\n",
        "et = ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
        "scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "# RandomForestClassifier와 결과가 비슷함. 대신 속도가 빠름"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fU9ZuvqNRRCF",
        "outputId": "b2154474-338f-4a9f-882d-7772712aacda"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9974503966084433 0.8887848893166506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "et.fit(train_input, train_target)\n",
        "print(et.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKSqLfbdTF09",
        "outputId": "d4c4e511-ff0b-43b7-88ce-7a35e6ef3ea7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.20183568 0.52242907 0.27573525]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [0.12345626 0.86862934 0.0079144 ] LogisticRegression 이전 결과"
      ],
      "metadata": {
        "id": "_IR2gfRiTOtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 그레이디언트 부스팅 Gradient Boosting(기울기)\n",
        "# 깊이가 얕은 결정 트리를 사용하여 이전 트리의 오차를 보완하는 방식으로 앙상블\n",
        "# 사이킷 런의 GradientBoostingClassifier는 기본적으로 깊이가 3인 결정 트리를 100개 사용\n",
        "# 때문에 과대적합에 강하고 일반적인 높은 일반화 성능을 기대함"
      ],
      "metadata": {
        "id": "3QyBidzoTQ00"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient는 경사 하강법을 사용하여 트리를 앙상블에 추가함.\n",
        "# 분류 : 로지스틱 손실 함수, 회귀에서는 평균 제급 오차 함수를 사용\n",
        "# 경사 하강법 손실 함수를 산으로 정의하고 가장 낮은 곳으로 찾아 내려오는 과정\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier # GradientBoostingClassifier 그레디언트 부스팅\n",
        "\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "# 과대 적합 해결"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPAkw4vYTnEw",
        "outputId": "7ab14e41-ea8b-4f3e-dfdb-51436a00e95d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8881086892152563 0.8720430147331015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)\n",
        "scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score'])) # 교차 검증 점수 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHGQPvi8U8Nl",
        "outputId": "c20d3d3f-96a2-4fd1-f721-6ed8a4c2f27d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9464595437171814 0.8780082549788999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gb.fit(train_input, train_target) # 훈련시작\n",
        "print(gb.feature_importances_) # 당도에 의존도가 낮음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVFkjSeNVDJw",
        "outputId": "c634b770-614d-4f8b-bc4b-d0479d294989"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.15887763 0.6799705  0.16115187]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 히스토그램 기반 그레이언트 부스팅 Histogram Gradient\n",
        "# https://goatlab.tistory.com/entry/Machine-Learning-Histogram-Based-Gradient-Boosting-Ensembles\n",
        "\n",
        "# 입력 특성을 256 구간으로 나눔 -> 노드를 분할할 때 최적의 분할을 매우 빠르게 찾을 수 있다.\n",
        "# 특히 256구간 중에서 하나를 떼어 놓고 누락된 값을 위해서 사용함\n",
        "# HistGradientBoostingClassifier는 기본 매개변수에서 안정적인 성능을 얻을 수있다.\n",
        "# HistGradientBoostingClassifier에는 트리의 개수를 지정하는데 n_estimators 대신 max_iter를 사용함(성능 항샹용)\n",
        "\n",
        "# from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "hgb = HistGradientBoostingClassifier(random_state=42)\n",
        "scores = cross_validate(hgb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score'])) # 과대 적합 억제 성공!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tTHgLrbVTvO",
        "outputId": "2475b208-f666-4794-9c0d-d3b69a1b5bda"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9321723946453317 0.8801241948619236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "# 특성 중요도 확인\n",
        "hgb.fit(train_input, train_target)\n",
        "result = permutation_importance(hgb, train_input, train_target, n_repeats=10, #n_repeats=10랜덤하게 섞을수 있는 회수 기본5\n",
        "                                random_state=42, n_jobs=-1)\n",
        "print(result.importances_mean)\n",
        "# 중요도,   평균,      표준편차"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_PL-cOGXCdu",
        "outputId": "98ff56ba-471c-4f07-a236-1ddc92da0976"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.08876275 0.23438522 0.08027708]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = permutation_importance(hgb, test_input, test_target, n_repeats=10,\n",
        "                                random_state=42, n_jobs=-1)\n",
        "print(result.importances_mean)\n",
        "# 테스트 세트에서 특성 중요도\n",
        "# 중요도,   평균,      표준편차"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyDDs0ncZRRf",
        "outputId": "fbf47db2-7ccd-4838-89dd-d9d0283b93ed"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.05969231 0.20238462 0.049     ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hgb.score(test_input, test_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PsOjEjkZmsH",
        "outputId": "6d218946-1815-49d8-a0b8-44c3303de6bc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8723076923076923"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HistGradientBoostingGegressor 히스토그램 기반 그레이디언트 부스팅의 회귀 버전\n",
        "# 사이킷 말고도 그레이디언트 부스팅 알고리즘을 구현한 라이브러리가 다수 존재\n",
        "# XGBoost 대표적임\n",
        "from xgboost import XGBClassifier\n",
        "# tree_method='hist' 히스토그램 기반 그레이디언트 부스팅용\n",
        "xgb = XGBClassifier(tree_method='hist', random_state=42)\n",
        "scores = cross_validate(xgb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Pl6lixBZxlu",
        "outputId": "a39dcffc-763c-43f9-dfa8-7329ed87fc0e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9558403027491312 0.8782000074035686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LGBMClassifier ms에서 만든 LightGBM^2\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "lgb = LGBMClassifier(random_state=42)\n",
        "scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tULSTxFYaawP",
        "outputId": "95f52342-e5ba-48c9-a8af-315e4afd7abe"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.935828414851749 0.8801251203079884\n"
          ]
        }
      ]
    }
  ]
}